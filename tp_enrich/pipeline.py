"""\nCore enrichment pipeline - reusable function for CLI and API\nExtracted from main.py to enable both CLI and API access\n"""\nimport os\nimport json\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Optional, Any\nfrom urllib.parse import urlparse\nfrom .logging_utils import setup_logger\nfrom .io_utils import load_input_csv, write_output_csv, get_output_schema\nfrom .classification import classify_name\nfrom .normalization import normalize_business_name, ensure_company_search_name\nfrom .dedupe import identify_unique_businesses, get_enrichment_context\nfrom .cache import EnrichmentCache\nfrom . import local_enrichment\nfrom .email_enrichment import enrich_emails_for_domain\nfrom .phone_enrichment import enrich_business_phone_waterfall\nfrom .merge_results import merge_enrichment_results\n\nlogger = setup_logger(__name__)\n\n\ndef _safe_str(x):\n    """Safely convert value to string, handling None and NaN."""\n    if x is None:\n        return ""\n    try:\n        if pd.isna(x):\n            return ""\n    except Exception:\n        pass\n    s = str(x).strip()\n    return s\n\n\ndef _norm_key(x: str) -> str:\n    """Normalize key: keep it simple + stable; match how company_search_name is generated."""\n    if x is None or (isinstance(x, float) and np.isnan(x)):\n        return ""\n    s = str(x).strip().lower()\n    # light normalization to make join stable\n    s = " ".join(s.split())\n    return s\n\n\ndef _get(res, *keys, default=None):\n    """\n    Works with:\n      - dict results\n      - dataclass/obj results (attributes)\n      - nested dicts/objs\n    """\n    cur = res\n    for k in keys:\n        if cur is None:\n            return default\n        # dict\n        if isinstance(cur, dict):\n            cur = cur.get(k, None)\n        else:\n            cur = getattr(cur, k, None)\n    return default if cur is None else cur\n\n\ndef merge_enrichment_back_to_rows(df: pd.DataFrame, enriched_businesses: list) -> pd.DataFrame:\n    """\n    PATCH: Robust merge-back function\n\n    df: row-level dataframe (all rows including business + person + other)\n    enriched_businesses: list of dicts, each dict contains business-level enrichment fields\n\n    Returns df with enrichment columns filled.\n\n    Key improvements:\n    - Uses company_search_name as primary join key (works even if company_normalized_key is NaN)\n    - Handles missing columns gracefully\n    - Deduplicates businesses by join key (keeps best record by data completeness)\n    - Forces object dtype to avoid pandas FutureWarning\n    """\n    logger.info("Step 6: Merging enrichment results back to rows...")\n\n    if df is None or df.empty:\n        logger.warning("  Empty dataframe, nothing to merge")\n        return df\n\n    # Ensure join columns exist\n    if "company_search_name" not in df.columns:\n        # fall back to raw_display_name if that exists\n        if "raw_display_name" in df.columns:\n            df["company_search_name"] = df["raw_display_name"]\n        else:\n            logger.error("  Missing company_search_name and raw_display_name columns")\n            raise ValueError("merge_enrichment_back_to_rows: missing company_search_name and raw_display_name")\n\n    # Create a stable join key on rows\n    df = df.copy()\n    df["_join_key"] = df["company_search_name"].apply(_norm_key)\n\n    # Build business-level table\n    biz_df = pd.DataFrame(enriched_businesses or [])\n    if biz_df.empty:\n        # Nothing to merge\n        logger.info("  No enrichment results to merge")\n        return df.drop(columns=["_join_key"], errors="ignore")\n\n    # Create stable join key on businesses\n    # Prefer company_search_name; fallback to company_normalized_key; fallback to company_name\n    for cand in ["company_search_name", "company_normalized_key", "company_name", "name"]:\n        if cand in biz_df.columns:\n            biz_df["_join_key"] = biz_df[cand].apply(_norm_key)\n            # If we got at least some non-empty keys, use it\n            if (biz_df["_join_key"] != "").any():\n                break\n    if "_join_key" not in biz_df.columns:\n        biz_df["_join_key"] = ""\n\n    logger.info("  Built enrichment index for %d businesses", len(biz_df))\n\n    # Deduplicate businesses by join key (keep best record: prefer ones with phone/email)\n    def _score_row(r):\n        score = 0\n        if str(r.get("primary_phone") or "").strip(): score += 2\n        if str(r.get("primary_email") or "").strip(): score += 2\n        if str(r.get("business_address") or "").strip(): score += 1\n        if str(r.get("company_domain") or "").strip(): score += 1\n        return score\n\n    biz_df["_score"] = biz_df.apply(_score_row, axis=1)\n    biz_df = biz_df.sort_values(["_join_key", "_score"], ascending=[True, False])\n    biz_df = biz_df.drop_duplicates(subset=["_join_key"], keep="first").drop(columns=["_score"])\n\n    logger.info("  After deduplication: %d unique businesses", len(biz_df))\n\n    # Columns we want to bring back (only if present in business table)\n    wanted = [\n        "company_domain",\n        "domain_confidence",\n        "primary_phone",\n        "primary_phone_display",\n        "primary_phone_source",\n        "primary_phone_confidence",\n        "primary_email",\n        "primary_email_type",\n        "primary_email_source",\n        "primary_email_confidence",\n        "business_address",\n        "business_city",\n        "business_state_region",\n        "business_postal_code",\n        "business_country",\n        "all_phones_json",\n        "generic_emails_json",\n        "person_emails_json",\n        "catchall_emails_json",\n        "overall_lead_confidence",\n        "enrichment_status",\n        "enrichment_notes",\n    ]\n    present = [c for c in wanted if c in biz_df.columns]\n\n    logger.info("  Merging %d enrichment columns: %s", len(present), present)\n\n    # Prepare minimal merge frame\n    merge_df = biz_df[["_join_key"] + present].copy()\n\n    # Merge\n    out = df.merge(merge_df, on="_join_key", how="left", suffixes=("", "_biz"))\n\n    # IMPORTANT: pandas dtype safety (avoid FutureWarning & "incompatible dtype" issues)\n    # Ensure target columns are object so assignment doesn't silently fail / coerce weirdly.\n    for c in present:\n        if c not in out.columns:\n            out[c] = ""\n        out[c] = out[c].astype("object")\n\n    # Cleanup\n    out = out.drop(columns=["_join_key"], errors="ignore")\n\n    logger.info("  Finished merging enrichment results into %d rows", len(out))\n    return out\n\n\ndef _compute_confidence(row: Dict[str, Any]) -> str:\n    """Compute overall confidence based on phone and email presence."""\n    has_phone = bool(row.get("primary_phone"))\n    has_email = bool(row.get("primary_email"))\n    if has_phone and has_email:\n        return "high"\n    if has_phone or has_email:\n        return "medium"\n    return "failed"\n\n\ndef enrich_single_business(name: str, region: str | None = None) -> Dict[str, Any]:\n    """\n    PHASE 2: Enhanced enrichment function with phone waterfall.\n\n    Flow:\n    1. Google Places for address/website (phone extracted but not used directly)\n    2. Extract domain from website\n    3. Phone waterfall (Google → Yelp → Website → Apollo) with validation\n    4. Email enrichment (Hunter only)\n    5. Compute overall confidence\n\n    Args:\n        name: Business name\n        region: Optional region/location\n\n    Returns:\n        Dict with enriched business data including validated phone\n    """\n    logger.info("Enriching business: %s", name)\n\n    row = {\n        "business_name": name,\n        "primary_phone": None,\n        "phone": None,\n        "primary_email": None,\n        "email": None,\n        "emails": [],\n        "email_source": None,\n        "website": None,\n        "domain": None,\n        "address": None,\n        "city": None,\n        "state_region": None,\n        "postal_code": None,\n        "country": None,\n    }\n\n    # ---- LOCAL (Google Places only)\n    local = local_enrichment.enrich_local_business(name, region)\n\n    # Store address/location data\n    if local:\n        for f in ["address", "city", "state_region", "postal_code", "country"]:\n            if local.get(f):\n                row[f] = local[f]\n\n        if local.get("website"):\n            row["website"] = local["website"]\n\n    # ---- DOMAIN EXTRACT\n    website = row.get("website")\n    if website:\n        try:\n            parsed = urlparse(website)\n            host = parsed.netloc or parsed.path\n            if host.startswith("www."):\n                host = host[4:]\n            row["domain"] = host.lower()\n        except Exception:\n            row["domain"] = None\n\n    domain = row.get("domain")\n\n    # ---- PHONE WATERFALL (Google → Yelp → Website → Apollo)\n    phone_layer = enrich_business_phone_waterfall(\n        biz_name=name,\n        google_hit=local or {},\n        domain=domain\n    )\n\n    # Map phone results\n    row["primary_phone"] = phone_layer.get("primary_phone")\n    row["phone"] = phone_layer.get("primary_phone")\n    row["phone_source"] = phone_layer.get("primary_phone_source")\n    row["phone_confidence"] = phone_layer.get("primary_phone_confidence")\n    row["all_phones_json"] = phone_layer.get("all_phones_json")\n\n    # ---- EMAIL ENRICHMENT (Hunter only)\n    logger.info(f"========== EMAIL WATERFALL START: domain='{domain}' ==========")\n    try:\n        email_data = enrich_emails_for_domain(domain)\n        if email_data.get("primary_email"):\n            row["primary_email"] = email_data["primary_email"]\n            row["email"] = email_data["primary_email"]\n            row["emails"] = email_data.get("emails") or []\n            row["email_source"] = email_data.get("email_source")\n            logger.info(f"  EMAIL HIT: source={email_data.get('email_source')} email={email_data.get('primary_email')}")\n        else:\n            logger.info(f"  EMAIL MISS: no email found for domain '{domain}'")\n    except Exception as e:\n        logger.error(f"  EMAIL ERROR: {repr(e)}")\n        # Don't crash pipeline on email failures\n    logger.info(f"========== EMAIL WATERFALL END: primary_email={row.get('primary_email')} ==========")\n\n    row["confidence"] = _compute_confidence(row)\n    return row\ndef enrich_business(business_info: Dict, cache: EnrichmentCache) -> Dict:\n    """\n    Enrich a single business through all sources\n    Args:\n        business_info: Business information dict\n        cache: Enrichment cache\n    Returns:\n        Complete enrichment result\n    """\n    normalized_key = business_info['company_normalized_key']\n    company_name = business_info['company_search_name']\n\n    # Check cache\n    if cache.has(normalized_key):\n        logger.info(f"  -> Using cached result for {company_name}")\n        return cache.get(normalized_key)\n\n    # Get enrichment context\n    context = get_enrichment_context(business_info)\n    region = context.get('state') or context.get('region')\n\n    # Use the simplified enrichment function\n    enriched_data = enrich_single_business(company_name, region=region)\n\n    # --- BUILD ENRICHED ROW (DO NOT CHANGE INDENTATION) ---\n    enriched_row = {\n        # identity / keys\n        "company_normalized_key": enriched_data.get("company_normalized_key", normalized_key),\n        "company_search_name": enriched_data.get("company_search_name", company_name),\n        "company_domain": enriched_data.get("company_domain") or enriched_data.get("domain"),\n        "domain_confidence": enriched_data.get("domain_confidence", "high" if enriched_data.get("domain") else "none"),\n\n        # primary phone\n        "primary_phone": enriched_data.get("primary_phone"),\n        "primary_phone_display": enriched_data.get("primary_phone"),\n        "primary_phone_source": enriched_data.get("primary_phone_source") or enriched_data.get("phone_source"),\n        "primary_phone_confidence": enriched_data.get("primary_phone_confidence") or enriched_data.get("phone_confidence"),\n\n        # primary email\n        "primary_email": enriched_data.get("primary_email"),\n        "primary_email_type": enriched_data.get("primary_email_type", "generic"),\n        "primary_email_source": enriched_data.get("primary_email_source") or enriched_data.get("email_source"),\n        "primary_email_confidence": enriched_data.get("primary_email_confidence", "medium" if enriched_data.get("primary_email") else "none"),\n\n        # address\n        "business_address": enriched_data.get("business_address") or enriched_data.get("address"),\n        "business_city": enriched_data.get("business_city") or enriched_data.get("city"),\n        "business_state_region": enriched_data.get("business_state_region") or enriched_data.get("state_region"),\n        "business_postal_code": enriched_data.get("business_postal_code") or enriched_data.get("postal_code"),\n        "business_country": enriched_data.get("business_country") or enriched_data.get("country"),\n\n        # metadata\n        "overall_lead_confidence": enriched_data.get("overall_lead_confidence", enriched_data.get("confidence", "medium")),\n        "enrichment_status": enriched_data.get("enrichment_status", "success"),\n        "enrichment_notes": enriched_data.get("enrichment_notes", ""),\n\n        # debug payloads\n        "all_phones_json": enriched_data.get("all_phones_json"),\n        "generic_emails_json": enriched_data.get("generic_emails_json"),\n        "person_emails_json": enriched_data.get("person_emails_json"),\n        "catchall_emails_json": enriched_data.get("catchall_emails_json"),\n        "source_platform": "trustpilot",\n    }\n    # --- END BUILD ENRICHED ROW ---\n\n    # Save to cache\n    cache.set(normalized_key, enriched_row)\n    logger.info(f"  -> Completed enrichment for {company_name} (confidence: {enriched_row.get('overall_lead_confidence')})")\n    return enriched_row\ndef run_pipeline(\n    input_csv_path: str,\n    output_csv_path: str,\n    cache_file: str = "enrichment_cache.json",\n    config: Optional[Dict] = None\n) -> Dict:\n    """\n    Core enrichment pipeline - reusable function for CLI and API\n    Args:\n        input_csv_path: Path to input Trustpilot CSV\n        output_csv_path: Path to output enriched CSV\n        cache_file: Path to enrichment cache file\n        config: Optional configuration dict (e.g., lender_name_override)\n    Returns:\n        Dict with statistics about the run\n    """\n    logger.info("="*60)\n    logger.info("Starting Trustpilot Enrichment Pipeline")\n    logger.info("="*60)\n    # ENV VAR CHECK (helps debug why providers aren't running)\n    logger.info(\n        f"ENV CHECK: HUNTER_KEY={'yes' if os.getenv('HUNTER_API_KEY') else 'no'} | "\n        f"SNOV_ID={'yes' if os.getenv('SNOV_CLIENT_ID') else 'no'} | "\n        f"SNOV_SECRET={'yes' if os.getenv('SNOV_CLIENT_SECRET') else 'no'} | "\n        f"APOLLO_KEY={'yes' if os.getenv('APOLLO_API_KEY') else 'no'}"\n    )\n    config = config or {}\n    # Load input CSV\n    logger.info("Step 1: Loading input CSV...")\n    df = load_input_csv(input_csv_path)\n    # Add row_id if not present\n    if 'row_id' not in df.columns:\n        df['row_id'] = range(1, len(df) + 1)\n    # Classify each row\n    logger.info("Step 2: Classifying display names...")\n    # Note: raw_display_name is already mapped in load_input_csv()\n    df['name_classification'] = df['raw_display_name'].apply(classify_name)\n    classification_counts = df['name_classification'].value_counts()\n    logger.info(f"  Classification results: {dict(classification_counts)}")\n    # Normalize business names\n    logger.info("Step 3: Normalizing business names...")\n    business_mask = df['name_classification'] == 'business'\n    df.loc[business_mask, ['company_search_name', 'company_normalized_key']] = df.loc[business_mask, 'raw_display_name'].apply(\n        lambda x: pd.Series(normalize_business_name(x))\n    )\n    # Ensure company_search_name is populated for all business rows\n    df = ensure_company_search_name(df)\n    # Debug logging\n    logger.info(\n        "Post-normalization: business rows=%s, business with company_search_name=%s",\n        int((df["name_classification"] == "business").sum()),\n        int(\n            (\n                (df["name_classification"] == "business")\n                & df["company_search_name"].notna()\n                & (df["company_search_name"].astype("string").str.strip() != "")\n            ).sum()\n        ),\n    )\n    # Dedup by company_normalized_key\n    logger.info("Step 4: Identifying unique businesses...")\n    unique_businesses = identify_unique_businesses(df)\n    logger.info(f"  Found {len(unique_businesses)} unique businesses to enrich")\n    # Initialize cache\n    cache = EnrichmentCache(cache_file)\n    # Enrich each unique business\n    logger.info("Step 5: Enriching businesses...")\n    enrichment_results = {}\n    for idx, (normalized_key, business_info) in enumerate(unique_businesses.items(), 1):\n        logger.info(f"  [{idx}/{len(unique_businesses)}] Processing: {business_info['company_search_name']}")\n        try:\n            result = enrich_business(business_info, cache)\n            enrichment_results[normalized_key] = result\n        except Exception as e:\n            logger.error(f"  Error enriching {business_info['company_search_name']}: {e}")\n            enrichment_results[normalized_key] = {\n                'enrichment_status': 'error',\n                'enrichment_notes': str(e),\n                'overall_lead_confidence': 'failed'\n            }\n    # Save cache\n    cache.save_cache()\n    # Calculate enrichment statistics\n    logger.info("="*60)\n    logger.info("ENRICHMENT SUMMARY")\n    logger.info("="*60)\n    logger.info(f"  Total unique businesses processed: {len(enrichment_results)}")\n    # Count results with domains, phones, emails\n    with_domain = sum(1 for r in enrichment_results.values() if r.get('company_domain'))\n    with_phone = sum(1 for r in enrichment_results.values() if r.get('primary_phone'))\n    with_email = sum(1 for r in enrichment_results.values() if r.get('primary_email'))\n    logger.info(f"  Businesses with domain: {with_domain}/{len(enrichment_results)}")\n    logger.info(f"  Businesses with phone: {with_phone}/{len(enrichment_results)}")\n    logger.info(f"  Businesses with email: {with_email}/{len(enrichment_results)}")\n    # Count by confidence\n    conf_counts = {}\n    for r in enrichment_results.values():\n        conf = r.get('overall_lead_confidence', 'unknown')\n        conf_counts[conf] = conf_counts.get(conf, 0) + 1\n    logger.info(f"  Confidence breakdown: {conf_counts}")\n    logger.info("="*60)\n\n    # Merge back to rows using robust matching function\n    # Convert dict values to list for new merge function signature\n    df = merge_enrichment_back_to_rows(df, list(enrichment_results.values()))\n    # Map source columns from input\n    if 'url' in df.columns:\n        df['source_review_url'] = df['url']\n    if 'date' in df.columns or 'review_date' in df.columns:\n        df['review_date'] = df.get('date', df.get('review_date'))\n    if 'rating' in df.columns or 'stars' in df.columns:\n        df['review_rating'] = df.get('rating', df.get('stars'))\n    df['source_platform'] = 'trustpilot'\n    # Apply lender name override if provided\n    if config.get('lender_name_override'):\n        df['source_lender_name'] = config['lender_name_override']\n    else:\n        # Extract lender name from URL if available\n        if 'source_review_url' in df.columns:\n            df['source_lender_name'] = df['source_review_url'].apply(\n                lambda x: x.split('/')[3] if isinstance(x, str) and '/' in x else None\n            )\n    # Write final CSV\n    logger.info("Step 7: Writing output CSV...")\n    output_schema = get_output_schema()\n    write_output_csv(df, output_csv_path, output_schema)\n    # Calculate statistics\n    stats = {\n        'total_rows': len(df),\n        'businesses': int(classification_counts.get('business', 0)),\n        'persons': int(classification_counts.get('person', 0)),\n        'others': int(classification_counts.get('other', 0)),\n        'unique_businesses': len(unique_businesses),\n        'enriched': len([r for r in enrichment_results.values() if r.get('enrichment_status') != 'error']),\n        'errors': len([r for r in enrichment_results.values() if r.get('enrichment_status') == 'error'])\n    }\n    logger.info("="*60)\n    logger.info("Pipeline completed successfully!")\n    logger.info(f"  Total rows: {stats['total_rows']}")\n    logger.info(f"  Businesses: {stats['businesses']}")\n    logger.info(f"  Unique businesses enriched: {stats['enriched']}/{stats['unique_businesses']}")\n    logger.info(f"  Output file: {output_csv_path}")\n    logger.info("="*60)\n    return stats\n